{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b0a12a",
   "metadata": {},
   "source": [
    "# CS 375 Homework 2: Naive Bayes for Text Classification\n",
    "The goal of this assignment is to give you practice implementing a Naive Bayes classifier and evaluate the classifier's performance on real-world datasets. \n",
    "\n",
    "You'll apply your Naive Bayes classifier to two datasets: messages for disaster relief (`triage`) and sentiment about COVID-19 (`covid`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccdcf0c",
   "metadata": {},
   "source": [
    "## Organization and Instructions \n",
    "\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins. \n",
    "\n",
    "**Part 1: Background.**\n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "- 1C. Tips \n",
    "\n",
    "**Part 2: Your implementation.** This is where you will implement your solution by modifying the following four functions within the `NaiveBayesClassifier()` class: \n",
    "- `__init__()`\n",
    "- `train()`\n",
    "- `predict()`\n",
    "- `get_prob_label_given_word()` \n",
    "\n",
    "**Part 3: Evaluation on real datasets.** In the third part, you will evaluate your NaiveBayesClassifier on two real-world datasets. \n",
    "- 3A. You will train and evaluate on the `triage` data. \n",
    "- 3B. You will inspect words with the highest predicted probability for each label. \n",
    "- 3C. You will evaluate your trained classifier on the `covid` dataset. You will have to <span style=\"color:blue\">answer a free-response question</span> about your classifier on this dataset. \n",
    "- 3D. Ethical considerations: You will <span style=\"color:blue\">answer a free-response question</span> about the ethics of using your classifier. \n",
    "\n",
    "**(Optional) Part 4: Extra Credit.** Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission. \n",
    "\n",
    "**Addtional instructions.** \n",
    "- Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "- However, you can talk to classmates about *high-level* approaches. In the **Process Reporting** section, record the names of any classmates you spoke with about this assignment. \n",
    "\n",
    "**Evaluation.**\n",
    "\n",
    "For HW1, your solution was evaluated on F1, the harmonic mean of precision and recall. That metric is still important for text classification; however, on this assignment you will be evaluated on `accuracy`, the number of correct predictions divided by the total number of predictions. \n",
    "\n",
    "From your classifier's predictions, we count the number of `true positives (tp)`, `false positives (fp)`, `false negatives (fn)` and `true negatives (tn)` and calculate `accuracy` as \n",
    "$$ \\text{accuracy} = \\frac{tp+tn}{tp+tn+fp+fn} $$\n",
    "\n",
    "Unlike rule-based systems (e.g., regular expressions) in which perfect accuracy is within rearch, it is often difficult to obtain perfect accuracy with text classifiers. \n",
    "\n",
    "Our reference implementation of Naive Bayes achieves the following `accuracy` scores on the `triage` dataset splits:  \n",
    "- train : `0.829`\n",
    "- dev: `0.733`\n",
    "- test: `0.763`\n",
    "\n",
    "**Grading.**\n",
    "\n",
    "- **20 points (autograded):** This portion of your grade reflects how well your submission performs on the `training set` of the `triage` dataset compared to our reference implementation metrics. Your points are calculated as \n",
    "    ```\n",
    "    (1-(0.829 - min(accuracy on train, 0.829))/0.829) * 20 points \n",
    "    ``` \n",
    "    \n",
    "- **20 points (autograded):** This portion of your grade reflects how well your submission performs on the `dev set` of the `triage` dataset compared to our reference implementation metrics.  Your points are calculated as \n",
    "    ```\n",
    "    (1 -(0.733 - min(accuracy on dev, 0.733))/0.733) * 20 points \n",
    "    ```\n",
    "- **20 points (autograded):** This portion of your grade reflects how well your submission performs on the `test set` of the `triage` dataset compared to our reference implementation metrics. You will not have access to the test set but will be able to see your score on Gradescope. Your points are calculated as   \n",
    "    ```\n",
    "    (1-(0.763 - min(accuracy on test, 0.763))/0.763) * 20 points \n",
    "    ``` \n",
    "    \n",
    "- **10 points (autograded):** The autograder will randomly sample words from the training corpus and evaluate your  `get_prob_label_given_word()`. \n",
    "- **5 points (manually graded):** TAs and the instructor will evaluate your response to Part 3C. \n",
    "- **5 points (manually graded):** TAs and the instructor will evaluate your response to Part 3D. \n",
    "\n",
    "**Submission.** \n",
    "Once you have completed Parts 1, 2 and 3, run the final cell in this notebook. This will create `submission.zip` which you will then upload to Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6bad0",
   "metadata": {},
   "source": [
    "## 1A. Environment set-up\n",
    "\n",
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29e6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2d631",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ad8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for this assignment \n",
    "# Do not modify this cell \n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "import random\n",
    "from typing import List, Dict, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import * #helper functions for this assignment located in util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2f06e",
   "metadata": {},
   "source": [
    "**Note:** In this assignment, you are **NOT** allowed to import or use any other packages outside the Python standard and the ones we imported above.\n",
    "\n",
    "This means you should not use `spaCy`, `NLTK`, `gensim`, or `scikit-learn`, even though those are provided in the conda environment we set up for you. If your solution uses any such extra dependencies it will fail the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c5ae8",
   "metadata": {},
   "source": [
    "## 1B. Data exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea64190",
   "metadata": {},
   "source": [
    "As is typical when dealing with real-world data, the first thing we need to do is understand and characterize our data. For the `triage` dataset in this assignment, we provide you with approximately `26K` documents from several major disasters: \n",
    "- [Earthquake in Haiti (2010)](https://en.wikipedia.org/wiki/2010_Haiti_earthquake)\n",
    "\n",
    "- [Floods in Pakistan (2010)](https://en.wikipedia.org/wiki/2010_Pakistan_floods)\n",
    "\n",
    "- [Earthquake in Chile (2010)](https://en.wikipedia.org/wiki/2010_Chile_earthquake)\n",
    "\n",
    "- [Hurricane Sandy in North America (2012)](https://en.wikipedia.org/wiki/Hurricane_Sandy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b08ac0",
   "metadata": {},
   "source": [
    "Recall, our formulation of Naive Bayes from lecture uses $(x, y)$ pairs in which $x$ are documents and $y$ are class labels. \n",
    "\n",
    "For the `triage` dataset, these variables are as follows\n",
    "- *x:* The documents we will classify are either text messages, Twitter posts, or snippets from news articles during the disasters above. \n",
    "    - These have all been translated from their original language to English by humans. \n",
    "    - However, the translations are not perfect and we might have to work with \"messy\" data as we would encounter in real-world settings. \n",
    "    - If you are curious about the crowdsourcing translation effort for messages from Haiti in particular, feel free to check out [this paper](https://nlp.stanford.edu/pubs/munro2010translation.pdf).\n",
    "- *y:* Our class labels are annotations from humans from a crowdsourcing platform called CrowdFlower. \n",
    "    - *y=0* indicates this is a document that is not about aid, `not aid` \n",
    "    - *y=1* indicates this is a document about `aid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9869f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load this data via the function from util.py\n",
    "triage_dataset = load_data(\"./data/triage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4560b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'util.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# This is a custom-defined Dataset class, defined in util.py \n",
    "print(type(triage_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d31a8d",
   "metadata": {},
   "source": [
    "Both the `triage` and `covid` datasets are split into `train` and `dev` sets. Your solution will also be evaluated on a held-out `test` set via Gradescope, but as in real deployment settings you will not have access to the individual examples in the `test` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e2dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triage_dataset.train contains 21046 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"triage_dataset.train contains {len(triage_dataset.train)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05094834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triage_dataset.dev contains 2573 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"triage_dataset.dev contains {len(triage_dataset.dev)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0492e",
   "metadata": {},
   "source": [
    "Let's look at individual examples, which are from the custom class `Example` from `util.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbc2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'util.Example'>\n"
     ]
    }
   ],
   "source": [
    "print(type(triage_dataset.train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f66ef927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example 11107:\n",
      "Label(y) = 1\n",
      "Text = these were deployed in teams of five persons to provide minor surgical and primary healthcare in the area of destroyed rural health facilities\n",
      "\n",
      "Tokens = ['these', 'were', 'deployed', 'in', 'teams', 'of', 'five', 'persons', 'to', 'provide', 'minor', 'surgical', 'and', 'primary', 'healthcare', 'in', 'the', 'area', 'of', 'destroyed', 'rural', 'health', 'facilities']\n"
     ]
    }
   ],
   "source": [
    "# Re-run this cell several times to look at different examples \n",
    "random_index = random.randint(0, len(triage_dataset.train))\n",
    "print(f\"Training example {random_index}:\")\n",
    "print(f\"Label(y) = {triage_dataset.train[random_index].label}\")\n",
    "tokens = triage_dataset.train[random_index].words\n",
    "text = \" \".join(tokens)\n",
    "print(f\"Text = {text}\")\n",
    "print()\n",
    "print(f\"Tokens = {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e067b4",
   "metadata": {},
   "source": [
    "*Sanity check:* Does it make sense to you that the text above is about `aid` if `Label(y) = 1` or not about aid if `Label(y) = 0`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864b860",
   "metadata": {},
   "source": [
    "*Note:* The data you are given has already been preprocessed and tokenized -- all punctuation has been removed except for hashtags and apostrophes, and all text has been converted to lowercase. There is no need for any additional preprocessing on the text. In real-world applications, you almost always will have to implement these pre-processing steps yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1635a5",
   "metadata": {},
   "source": [
    "## 1C. Tips \n",
    "\n",
    "Here, we provide some suggestions and hints. It is possible to implement a working solution without following these suggestions, so feel free to ignore the ones that are not relevant to your approach. \n",
    "\n",
    "- **Use log probabilities.** As we discussed in class, implementing everything in log space can help avoid underflow. \n",
    "- **Keep track of the vocabulary.** For the purposes of implementing Laplace Smoothing (+1 smoothing), it may be helpful to keep track of the vocabulary, the set of all words you've seen in the training data. The Python `set()` structure may be useful here.\n",
    "- **Remember defaultdict and Counter.** You may find Python's [defaultdict](https://docs.python.org/3.8/library/collections.html#collections.defaultdict) and/or [Counter](https://docs.python.org/3.8/library/collections.html#collections.Counter) helpful in your implementation when counting.\n",
    "- **In Python, assignment is by reference.** Remember that in Python, assignment is by reference, not by value, for non-primitive types. More simply, when you're assigning an existing list or dict to a new variable, it does NOT make a copy. It just gives a reference to the existing list or dict.\n",
    "- **Unknown words in dev and test.** Words in the dev/test set that are not seen in your training set should be ignored in your Naive Bayes computations. \n",
    "- **Code length.** Our reference implementation is just under 100 lines of code, including the skeleton code. It's quite possible that you can make a working implementation in fewer lines. However, if your implementation is signficantly longer than 100 lines, that might be a sign that your implementations is more complicated than necessary. \n",
    "- **In-class implementation.** In may prove helpful to return to the language modeling code implementation we discussed together in class to think about some best practices in data structures for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df41af0",
   "metadata": {},
   "source": [
    "## 2. Your solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c729ff",
   "metadata": {},
   "source": [
    "Complete the implementation of the `NaiveBayesClassifier` below.\n",
    "\n",
    "You are welcome to create additional helper functions *within* the class if your implementation requires it. However, any functions you write outside of the `NaiveBayesClassifier` class cannot be accessed by the autograder and may cause it to fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the implementation below \n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Implements Naive Bayes Classifier \n",
    "    Includes Laplace smoothing (add-1 smoothing) during training \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # TODO: add other data structures needed for predict() or train()\n",
    "        # CODE START\n",
    "        pass \n",
    "        # CODE END\n",
    "\n",
    "    def train(self, examples: List[Example]) -> None:\n",
    "        \"\"\"\n",
    "        This function inputs a list of labeled examples and \n",
    "        trains the classifier via Naive Bayes.  \n",
    "        \n",
    "        Hints: \n",
    "            - Remember to use Laplace smoothing! \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END\n",
    "             \n",
    "    def predict(self, examples: List[Example]) -> List[int]:\n",
    "        \"\"\"\n",
    "        This function inputs a list of Examples and \n",
    "        predicts their labels using the learned classifier \n",
    "        \n",
    "        It returns as a list of int variables that are the predicted\n",
    "        labels (e.g. 0 or 1)\n",
    "            \n",
    "        Hints: \n",
    "            - Remember to use logs to prevent underflow!\n",
    "            - Remember to ignore words not seen during training \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END\n",
    "\n",
    "    def get_prob_label_given_word(self, label: int) -> Dict:\n",
    "        \"\"\"\n",
    "        This function returns a dictionary for which \n",
    "            - keys are each unigram word in the vocabulary  \n",
    "            - values are p(label|word) from the trained model where\n",
    "                label is the exact value of the label inputted as an argument above \n",
    "            \n",
    "        Note: this is NOT the same as p(word|label)\n",
    "        \n",
    "        Hint: You should have already calculated and stored p(word|label) and p(label). \n",
    "            How can you use Bayes rule to obtain p(label|word)? \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5045de2",
   "metadata": {},
   "source": [
    "#### Debugging on \"toy\" corpus \n",
    "\n",
    "Like most real-world NLP systems, it can be helpful to examine the correctness of our code on a small \"toy\" dataset that we can analytically calculate the answers for. We'll give you one here but in future assignments you'll develop these yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus (subset of the example we used during lecture)\n",
    "toks1 = ['fun', 'couple', 'love', 'love']\n",
    "label1 = 0 \n",
    "ex1 = Example(toks1, label1)\n",
    "\n",
    "toks2 = ['fast', 'furious', 'shoot']\n",
    "label2 = 1 \n",
    "ex2 = Example(toks2, label2)\n",
    "\n",
    "toks3 = ['couple', 'fly', 'fast', 'fun', 'fun']\n",
    "label3 = 0 \n",
    "ex3 = Example(toks3, label3)\n",
    "\n",
    "toks4 = ['fast', 'couple', 'shoot', 'fly', 'bomb']\n",
    "label4 = 1\n",
    "ex4 = Example(toks4, label4)\n",
    "\n",
    "toy_training_data = [ex1, ex2, ex3]\n",
    "toy_dev_data = [ex4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call to check your implementation \n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.train(toy_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e04fc",
   "metadata": {},
   "source": [
    "After training, you may want to examine some of the data structures you created in `__init__()` within your implementation of your `NaiveBayesClassifier()` to ensure it's correct. Feel free to add cells below to check this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call to check your predict implementation \n",
    "nb_classifier.predict(toy_dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb89bf9",
   "metadata": {},
   "source": [
    "Many students encounter errors in the cell above. \n",
    "\n",
    "*Hint:* If you have an error above, think about how you're handling the train and test vocabularies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031da861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the p(y=0|word) for every word in the vocab \n",
    "y0_probs = nb_classifier.get_prob_label_given_word(0)\n",
    "sorted(y0_probs.items(), key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_probs = nb_classifier.get_prob_label_given_word(1)\n",
    "sorted(y1_probs.items(), key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140ec0b",
   "metadata": {},
   "source": [
    "*Hint:* For `y0_probs` and `y1_probs` above, what should each word sum to?  Try testing this yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f0fa2",
   "metadata": {},
   "source": [
    "## 3. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bc43e",
   "metadata": {},
   "source": [
    "#### 3A. Accuracy \n",
    "Let's evaluate the accuracy of your implementation on the `triage` dataset. Our reference implementation obtains: \n",
    "```\n",
    "Accuracy (train): 0.82946878266654\n",
    "Accuracy (dev): 0.7329965021375826\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3890dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load triage data (again)\n",
    "triage_data = load_data(\"./data/triage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "# evaluate() is implemented in util.py \n",
    "# Inspecting it, you'll see it trains your classifier and then caculates accuracy \n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "evaluate(nb_classifier, triage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27e0f4",
   "metadata": {},
   "source": [
    "In the cell above, our reference implementation also takes about `3 seconds` to run. If your code is running significantly slower, we recommend returning to your implementation and thinking about how you might speed it up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa1dd6",
   "metadata": {},
   "source": [
    "#### 3B. Validation via `P(label|word)`\n",
    "\n",
    "When building NLP systems, it's often helpful to manually inspect aspects of your model and check if these match your intution about the problem.  \n",
    "\n",
    "Let's use `get_prob_label_given_word()` to examine your model's predictions for the words with the highest `p(label|word)` below. Do these make sense? \n",
    "\n",
    "The autograder will test your implementation on some additional test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a717880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "\n",
    "print('Aid (y=1), most probable words')\n",
    "print('==='*15)\n",
    "vocab_probs_positive = nb_classifier.get_prob_label_given_word(1)\n",
    "top_10 = sorted(vocab_probs_positive.items(), key=lambda kv: -kv[1])[:10]\n",
    "for word, prob in top_10:\n",
    "    print(\"{0:<18} prob = {1}\".format(word, np.round(prob, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "\n",
    "print('Not aid (y=0), most probable words')\n",
    "print('==='*15)\n",
    "vocab_probs_negative = nb_classifier.get_prob_label_given_word(0)\n",
    "top_10 = sorted(vocab_probs_negative.items(), key=lambda kv: -kv[1])[:10]\n",
    "for word, prob in top_10:\n",
    "    print(\"{0:<18} prob = {1}\".format(word, np.round(prob, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc203f7",
   "metadata": {},
   "source": [
    "#### 3C. Generalization. \n",
    "\n",
    "Let's try to use the classifier you just trained on another text classification test: predicting messages related to `COVID` as positive or negative sentiment. \n",
    "\n",
    "This dataset consists of `reddit` comments on posts related to the `COVID-19` pandemic in 2020.\n",
    "\n",
    "Here the y-labels consist of `0` for `negative` sentiment and `1` for `positive` sentiment.\n",
    "\n",
    "*Warning:* Because this is data scraped from the internet, there may be explicit or hateful content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd7d69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "# Load COVID dataset \n",
    "covid_data = load_data(\"./data/covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc19d0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID dev example 9767:\n",
      "Label(y) = 1\n",
      "Text = Considering how its going in Singapore yikesYoure right though Theres a very stoic presence bring shown but people are running around like chickens with the heads cut off behind the scenes\n",
      "\n",
      "Tokens = ['Considering', 'how', 'its', 'going', 'in', 'Singapore', 'yikesYoure', 'right', 'though', 'Theres', 'a', 'very', 'stoic', 'presence', 'bring', 'shown', 'but', 'people', 'are', 'running', 'around', 'like', 'chickens', 'with', 'the', 'heads', 'cut', 'off', 'behind', 'the', 'scenes']\n"
     ]
    }
   ],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "# Re-run this cell several times to look at different examples \n",
    "random_index = random.randint(0, len(covid_data.dev))\n",
    "print(f\"COVID dev example {random_index}:\")\n",
    "print(f\"Label(y) = {covid_data.dev[random_index].label}\")\n",
    "tokens = covid_data.dev[random_index].words\n",
    "text = \" \".join(tokens)\n",
    "print(f\"Text = {text}\")\n",
    "print()\n",
    "print(f\"Tokens = {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "# We apply your trained classifier to this new data and\n",
    "print(\"Accuracy on COVID (dev) =\", calculate_accuracy(covid_data.dev, nb_classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbc4c5",
   "metadata": {},
   "source": [
    "How well did your trained classifier do? If it didn't do well, why do you think it did not? What are possible steps forward? \n",
    "\n",
    "(There is no one \"correct\" answer for this question. We will evaluating your answer based on your thought process. We are expecting *at minimum* two complete sentences for full credit.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0949cdd",
   "metadata": {},
   "source": [
    "**Part 3C Answer:**\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da90a4",
   "metadata": {},
   "source": [
    "#### 3D. Ethical considerations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c68743",
   "metadata": {},
   "source": [
    "We've seen there are two types of errors a classifier can make: `false positives` and `false negatives`. For different applications, these types of errors have different consequences. \n",
    "\n",
    "Thinking about the `triage` dataset and task holistically, which types of errors do you think would potentially cause more harm? Why? \n",
    "\n",
    "(There is no one \"correct\" answer for this question. We will evaluating your answer based on your thought process. We are expecting *at minimum* three complete sentences for full credit.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb2022",
   "metadata": {},
   "source": [
    "**Part 3D Answer:**\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2c605",
   "metadata": {},
   "source": [
    "## (Optional) 4. Extra Credit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115e8d8",
   "metadata": {},
   "source": [
    "*Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682b5a5",
   "metadata": {},
   "source": [
    "Above, we have implemented the most basic version of Naive Bayes. Try to extend it and make it better. Some suggestions to try: \n",
    "- Remove stopwords (e.g., `from nltk.corpus import stopwords`) \n",
    "- Change counts to indicators \n",
    "- Your implementation above only had to work for binary classification. Write some test cases on a toy corpus to make sure it works for multi-class classificiation. \n",
    "- Follow the suggestions in J&M Ch.4.4 and add a feature that is counted whenever a word from that sentiment lexicon occurs (for evaluation on the `covid` dataset). \n",
    "- Change from unigrams to other n-grams (e.g. bigrams or trigrams) \n",
    "- For [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), make the smoothing parameter (which we set as 1 in Laplace smoothing) a hyperparameter. Tune this parameter on the development set.  \n",
    "- Lemmatize words first (e.g. `from nltk.stem import WordNetLemmatizer`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14bee856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement a new version or an extension of your previous class\n",
    "import nltk \n",
    "\n",
    "class ExtraNaiveBayesClassifier:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abde531",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70deea1",
   "metadata": {},
   "source": [
    "**Processing reporting.** Please record your answers to the questions below by writing directly in this Markdown cell.\n",
    "\n",
    "If you talked with any of your classmates on this assignment please list their names here:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*\n",
    "\n",
    "Approximately how much time did you spend on this assignment:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ff36d",
   "metadata": {},
   "source": [
    "**Download zip.** Once you're satsified with your solution, save this file and run the cell below to automatically zip your file. This will produce `submission.zip` in the same folder as this file (same folder as `hw2.ipynb`). \n",
    "\n",
    "Submit `submission.zip` to Gradescope. \n",
    "\n",
    "*Note:* This script assumes that you have the `zip` utility installed and you can use `bash` on your system. If the cell below does not work you may need to zip your file manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbe590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw2.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw2.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
